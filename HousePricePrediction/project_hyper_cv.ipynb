{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for linear\n",
    "# Linear Regression is algorithm that analyses a scatter of data points and attempts to find\n",
    "# a best-fit line that describes analysed data.\n",
    "# Linear Regression creates this line on x,y axis: y=mx+b   (where y-predictive value, m- coefficient, b-constant/intercept)\n",
    "# where m=(y2-y1)/(x2-x1)\n",
    "# and number of attributes correspondes to number of dimensions (2 values- 2 dimencional, 3 values- 3 dim and so on)\n",
    "\n",
    "# define model\n",
    "model = linear_model.LinearRegression()\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=42) # n_splits - number of folds, n_repeats - number of times CV will be repeated\n",
    "# define search space\n",
    "space = dict()\n",
    "space['fit_intercept'] = [True, False]\n",
    "space['positive'] = [True, False]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for KNR\n",
    "#If we have a variable with undefined class that represents a point on Xy-axis (in 2 dimencions) and is is displaced on some distance\n",
    "#from a multiple clusters of data points (each cluster represents a defined class), KNN\n",
    "#alorithm finds the cluster to which undefined variable is most likely related (measuring the distance to the closest defined points, \n",
    "#and picking the cluster with the biggest number of close points).\n",
    "#Number of points the algorithm counts from the nearby clusters is \"k\" (from the title KNN)\n",
    "# define model\n",
    "model = KNeighborsRegressor()\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1) # n_splits - number of folds, n_repeats - number of times CV will be repeated\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_neighbors'] = [*range(1,21,1)]         #Number of neighbors to use\n",
    "space['weights'] = ['uniform', 'distance',]         # All points in each neighborhood are weighted equally or  closer neighbors of a query point \n",
    "                                                    # will have a greater influence than neighbors which are further away.\n",
    "space['algorithm'] = ['ball_tree', 'kd_tree','brute']              #Algorithm used to compute the nearest neighbors, 'auto' decides which one is better\n",
    "space['leaf_size'] = [*range(20,41,5)]           #Leaf size passed to BallTree or KDTree\n",
    "space['p'] = [1,2]                  # When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2\n",
    "space['metric'] = ['minkowski','precomputed']             # Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2\n",
    "space['n_jobs'] = [1,-1]\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for SVR \n",
    "# super heavy computationaly, I used manual tuning for SVR. Random Search CV could be considered for this one\n",
    "# Attempts to create a hyperplane (something straight, a straight line between clusters of data, for example)\n",
    "# Hyperplane is a line between clusters of data points and is perpendicular to the beam towards hyperplane from\n",
    "# the closest point of each cluster. The distance between hyperplane and data points from each side should be exactly the same.\n",
    "# In our case we want to pick hyperplane with the biggest distance to datapoints, which will mean a bigger MARGIN.\n",
    "# In case if data is randomly distributed on an XY-axis, SVM adds additional dimension to try and draw a hyperplane\n",
    "# that will devide our data in to groups. This is done with the help of kernel (or just a function, which takes f(x1,x2) and returns x3)\n",
    "# Also there is a \"soft margin\", which alows some outliers between the clusters \n",
    "# define model\n",
    "model = SVR(kernel='poly',epsilon=0.02,C=0.033,shrinking=True)\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1) # n_splits - number of folds, n_repeats - number of times CV will be repeated\n",
    "# define search space\n",
    "space = dict()\n",
    "#space['kernel'] = ['linear','poly','rbf','sigmoid']         # rbf is default\n",
    "#space['C'] = [*np.arange(0.001,0.1,0.002)]                  # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "                                                            #Must be strictly positive. The penalty is a squared l2 penalty\n",
    "#space['epsilon']=[*np.arange(0.001,0.1,0.002)]              # It specifies the epsilon-tube within which \n",
    "                                                            #no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
    "#space['gamma']=[*np.arange(0.1,0.4,0.01)] \n",
    "space['coef0']=[*np.arange(0.0,0.01,0.001)] \n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='r2', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for RandomForest\n",
    "# define model\n",
    "model = RandomForestRegressor()\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1) # n_splits - number of folds, n_repeats - number of times CV will be repeated\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_estimators'] = [*range(50,200,10)]         # The number of trees in the forest\n",
    "space['criterion'] = ['squared_error','absolute_error','friedman_mse','poisson']         # The function to measure the quality of a split (about error)\n",
    "space['max_depth'] = [*range(1,100,5)]              #The maximum depth of the tree\n",
    "space['min_samples_split'] = [1,2]           # The minimum number of samples required to split an internal node\n",
    "space['min_samples_leaf'] = [1,2]                 #The minimum number of samples required to be at a leaf node\n",
    "space['max_features']=[*range(1,100,5)] # The number of features to consider when looking for the best split\n",
    "space['bootstrap']=[True,False]         # whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree\n",
    "space['random_state']=[1,None,42]\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)    #grid=exaustive search\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Ridge\n",
    "# The L2 norm loss function, also known as the least squares error (LSE), \n",
    "# is used to minimize the sum of the square of differences between the target value and the estimated value\n",
    "#   S= E(y-f(x))^2\n",
    "#less robust to outliers than L1 since error is squared\n",
    "# define model\n",
    "model = Ridge()\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1) # n_splits - number of folds, n_repeats - number of times CV will be repeated\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag','saga','lbfgs','sparse_cg']     #Solver to use in the computational routines\n",
    "space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]         #Constant that multiplies the L2 term, controlling regularization strength\n",
    "space['fit_intercept'] = [True, False]             #Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations\n",
    "     \n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Lasso\n",
    "# The L1 loss, also known as Absolute Error Loss, is the absolute difference between a prediction and the actual value,\n",
    "#  calculated for each example in a dataset. The aggregation of all these loss values is called the cost function, \n",
    "# where the cost function for L1 is commonly MAE (Mean Absolute Error).\n",
    "#   L1 for 1 sample is : y - f(x), MSE = E(y-f(x))/n_rows\n",
    "# more robust to the outliers than L2\n",
    "# define model\n",
    "model = Lasso()\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1) # n_splits - number of folds, n_repeats - number of times CV will be repeated\n",
    "# define search space\n",
    "space = dict()\n",
    "space['selection'] = ['cyclic','random']     #If set to ‘random’, a random coefficient is updated every iteration \n",
    "                                                #rather than looping over features sequentially by default.\n",
    "space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100,200,300,400]         #Constant that multiplies the L2 term, controlling regularization strength\n",
    "space['fit_intercept'] = [True, False]             #Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations\n",
    "  \n",
    "\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04db9ca018ba5c38a2ebecd8d8fe5707a441bb220b7197d3173f5cbbed10024d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
